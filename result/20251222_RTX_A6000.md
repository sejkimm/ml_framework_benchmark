# Benchmark Report — NVIDIA RTX A6000 (2025-12-22)

## Environment

- Command: `python3 main.py`
- Docker Image: `nvcr.io/nvidia/pytorch:25.11-py3`
- GPU: NVIDIA RTX A6000 (SMs: 84)
- torch: `2.10.0a0+b558c986e8.nv25.11`
- triton: `3.5.0`
- Benchmark: warmup=30, iters=200
- Backends: `torch`, `cuda_ext`, `triton`

## Results

### Matmul (fp16)

| M | N | K | torch (ms) | torch (TFLOPS) | triton (ms) | triton (TFLOPS) | speedup (torch/triton) |
|---:|---:|---:|---:|---:|---:|---:|---:|
| 1024 | 1024 | 1024 | 0.022 | 95.96 | 0.025 | 85.80 | 0.894 |
| 2048 | 2048 | 2048 | 0.169 | 101.54 | 0.200 | 85.83 | 0.845 |
| 4096 | 4096 | 4096 | 1.221 | 112.52 | 1.351 | 101.75 | 0.904 |
| 8192 | 8192 | 4096 | 4.510 | 121.91 | 6.946 | 79.15 | 0.649 |
| 8192 | 8192 | 8192 | 9.736 | 112.93 | 14.000 | 78.54 | 0.695 |
| 16384 | 4096 | 4096 | 4.473 | 122.90 | 7.045 | 78.03 | 0.635 |
| 16384 | 8192 | 4096 | 8.953 | 122.81 | 13.870 | 79.28 | 0.645 |

### Residual + RMSNorm (fp16)

| rows | hid | torch (ms) | cuda_ext (ms) | speedup | triton (ms) | speedup |
|---:|---:|---:|---:|---:|---:|---:|
| 4096 | 1024 | 0.137 | 0.040 | 3.425 | 0.040 | 3.473 |
| 4096 | 4096 | 0.517 | 0.214 | 2.415 | 0.227 | 2.279 |
| 8192 | 4096 | 1.016 | 0.420 | 2.420 | 0.455 | 2.236 |
| 16384 | 4096 | 2.002 | 0.837 | 2.391 | 0.915 | 2.187 |

### SwiGLU (fp16)

| rows | hid | torch (ms) | cuda_ext (ms) | speedup | triton (ms) | speedup |
|---:|---:|---:|---:|---:|---:|---:|
| 4096 | 1024 | 0.069 | 0.039 | 1.758 | 0.039 | 1.758 |
| 4096 | 4096 | 0.255 | 0.149 | 1.709 | 0.151 | 1.690 |
| 16384 | 4096 | 0.997 | 0.582 | 1.713 | 0.594 | 1.678 |

## Highlights

- GEMM (matmul fp16): torch (cuBLAS) is faster; triton is ~1.1×–1.6× slower across these shapes.
- Fused ops:
  - residual+rmsnorm: `cuda_ext`/triton are ~2.2×–3.5× faster than torch (best: 3.473×).
  - swiglu: `cuda_ext`/triton are ~1.68×–1.76× faster than torch.

## Notes

- Torch uses cuBLAS/cuBLASLt, so pure GEMM is often faster in torch.
- Triton and custom CUDA extensions shine more with kernel fusion.
- The first run includes JIT/compile cost, so warmup is required.

## Raw Output

```text
GPU: NVIDIA RTX A6000
torch: 2.10.0a0+b558c986e8.nv25.11, triton: 3.5.0
SM count: 84
bench: warmup=30, iters=200, backends=torch,cuda_ext,triton
--------------------------------------------------------------------------------

== matmul (fp16) ==
     M      N      K |  torch(ms) torch(TFLOPS) | triton(ms) triton(TFLOPS) |  speedup
--------------------------------------------------------------------------------
  1024   1024   1024 |      0.022         95.96 |      0.025          85.80 |    0.894
  2048   2048   2048 |      0.169        101.54 |      0.200          85.83 |    0.845
  4096   4096   4096 |      1.221        112.52 |      1.351         101.75 |    0.904
  8192   8192   4096 |      4.510        121.91 |      6.946          79.15 |    0.649
  8192   8192   8192 |      9.736        112.93 |     14.000          78.54 |    0.695
 16384   4096   4096 |      4.473        122.90 |      7.045          78.03 |    0.635
 16384   8192   4096 |      8.953        122.81 |     13.870          79.28 |    0.645

== residual + rmsnorm (fp16) ==
  rows    hid |  torch(ms) |   cuda(ms)  speedup | triton(ms)  speedup
--------------------------------------------------------------------------------
  4096   1024 |      0.137 |      0.040    3.425 |      0.040    3.473
  4096   4096 |      0.517 |      0.214    2.415 |      0.227    2.279
  8192   4096 |      1.016 |      0.420    2.420 |      0.455    2.236
 16384   4096 |      2.002 |      0.837    2.391 |      0.915    2.187

== swiglu (fp16) ==
  rows    hid |  torch(ms) |   cuda(ms)  speedup | triton(ms)  speedup
--------------------------------------------------------------------------------
  4096   1024 |      0.069 |      0.039    1.758 |      0.039    1.758
  4096   4096 |      0.255 |      0.149    1.709 |      0.151    1.690
 16384   4096 |      0.997 |      0.582    1.713 |      0.594    1.678
--------------------------------------------------------------------------------
Notes:
 - Torch uses cuBLAS/cuBLASLt, so pure GEMM is often faster in torch.
 - Triton and custom CUDA extensions shine more with kernel fusion.
 - The first run includes JIT/compile cost, so warmup is required.
```
